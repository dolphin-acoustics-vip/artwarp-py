#!/usr/bin/env python3
"""
Diagnostic: compare max contour length BEFORE vs AFTER resampling (Sarasota or any dir).

BEFORE (old behaviour): CSV loader returned tempres=None for all; CLI used default 0.01.
  So step = sample_interval / 0.01 = 1 (with --sample-interval 0.01), and
  new_length = 1 + (n-1)/1 = n. Resampling was a no-op; contours kept original lengths.
  Max contour length = max of raw contour lengths.

AFTER (current behaviour): Per-file tempres from Time column. step = sample_interval / tempres.
  tempres > sample_interval => upsampled (more points); tempres < sample_interval => downsampled (fewer).
  E.g. Sarasota: all tempres ~2.7–5.4 ms => all downsampled => max 569 → 304.

Usage (from artwarp-py root, with venv or PYTHONPATH=src):
  python scripts/contour_length_comparison.py ./contours-sarasota

*NOTE: This script is autogenerated and not done by the author. It is primarily used for debugging and development.
"""
import sys
from pathlib import Path

# Allow running from repo root with PYTHONPATH=src
sys.path.insert(0, str(Path(__file__).resolve().parent.parent / "src"))

from artwarp.io.loaders import load_contours
from artwarp.utils.resample import resample_contours


def main():
    if len(sys.argv) < 2:
        print("Usage: python scripts/contour_length_comparison.py <contour_dir>")
        sys.exit(1)
    dir_path = sys.argv[1]
    sample_interval = 0.01
    default_tempres = 0.01

    print(f"Loading: {dir_path}")
    contours, names, tempres_list = load_contours(
        dir_path, file_format="csv", return_tempres=True
    )
    n_contours = len(contours)

    # Original lengths (BEFORE = what you had when resampling was no-op)
    orig_lengths = [len(c) for c in contours]
    max_orig = max(orig_lengths)
    min_orig = min(orig_lengths)
    mean_orig = sum(orig_lengths) / n_contours

    # Tempres: replace None and <=0 with default (same as CLI)
    safe_tempres = []
    for t in tempres_list:
        if t is None:
            safe_tempres.append(default_tempres)
        else:
            v = float(t)
            safe_tempres.append(default_tempres if v <= 0 else v)

    # Tempres stats
    tr_min = min(safe_tempres)
    tr_max = max(safe_tempres)
    tr_mean = sum(safe_tempres) / n_contours
    n_with_time = sum(1 for t in tempres_list if t is not None and float(t) > 0)

    # AFTER resampling (current behaviour)
    resampled = resample_contours(contours, safe_tempres, sample_interval)
    new_lengths = [len(c) for c in resampled]
    max_new = max(new_lengths)
    min_new = min(new_lengths)
    mean_new = sum(new_lengths) / n_contours

    # Which contour(s) became longest
    idx_max_new = [i for i, L in enumerate(new_lengths) if L == max_new]
    examples = idx_max_new[:3]

    # Up vs down sampling: step = sample_interval / tempres; step > 1 => fewer points (down), step < 1 => more (up)
    n_down = sum(1 for i in range(n_contours) if new_lengths[i] < orig_lengths[i])
    n_up = sum(1 for i in range(n_contours) if new_lengths[i] > orig_lengths[i])
    n_same = n_contours - n_down - n_up

    print()
    print("=== BEFORE (old behaviour: no per-file tempres, default 0.01 for all) ===")
    print("  Resampling was effectively no-op: step=1, lengths unchanged.")
    print(f"  Max contour length:  {max_orig}")
    print(f"  Min contour length:  {min_orig}")
    print(f"  Mean contour length: {mean_orig:.1f}")
    print()
    print("=== AFTER (current behaviour: per-file tempres from Time column) ===")
    print(f"  Resampling to {sample_interval}s interval with per-file tempres.")
    print(f"  Max contour length:  {max_new}")
    print(f"  Min contour length:  {min_new}")
    print(f"  Mean contour length: {mean_new:.1f}")
    print(f"  Downsampled (shorter): {n_down}  Upsampled (longer): {n_up}  Unchanged: {n_same}")
    print()
    print("=== Tempres in this dataset ===")
    print(f"  Contours with valid time-derived tempres: {n_with_time} / {n_contours}")
    print(f"  tempres min: {tr_min:.6f} s  max: {tr_max:.6f} s  mean: {tr_mean:.6f} s")
    if tr_max > sample_interval:
        print(f"  Contours with tempres > {sample_interval} s get UPsampled (more points).")
    if tr_max < sample_interval and tr_min > 0:
        print(f"  All tempres < {sample_interval} s => all contours DOWNsampled (fewer points).")
    print()
    if examples:
        print("  Example contour(s) that ended up longest after resampling:")
        for i in examples:
            print(f"    {names[i]}: orig len={orig_lengths[i]}, tempres={safe_tempres[i]:.6f}, new len={new_lengths[i]}")
    print()
    print("=== Effect on training ===")
    print(f"  Max contour length: before {max_orig}, after {max_new} (ratio {max_new / max_orig:.2f}x).")
    if max_new > max_orig:
        print("  Training is heavier (longer contours). Use --max-contour-length to cap.")
    else:
        print("  Training is lighter (shorter contours after resampling).")
    print()
    rec = min(600, max(500, max_orig))
    print("=== Suggested --max-contour-length (only if you want to cap) ===")
    print(f"  Optional: --max-contour-length {rec} to limit worst-case length.")
    print("  Adjust to 500 if still slow, or omit for no cap.")


if __name__ == "__main__":
    main()
